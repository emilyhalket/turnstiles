{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTA Subway Turnstile Counts ( Work in Progress)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Wrangling\n",
    "To get started, I need to scrape the text file data from the [mta website](http://web.mta.info/developers/turnstile.html). After looking over the website and inspecting a couple of files, I noted some challenges to keep in mind when collecting and organizing the data:\n",
    "\n",
    "  + there are two different data types - before and after 10/18/14\n",
    "      * the older files have multiple entries per line, which will need to be separated\n",
    "      * the older files are also missing a few fields (station, linename, division)\n",
    "          - this information seems to be available in separate file, although not entirely complete\n",
    "      * the older files are formatted using mm-dd-yy while the new files are mm/dd/yy\n",
    "  + there are 52 files per year, which each hold a week's worth of data\n",
    "  + to make working with the data more manageable, use sqlite with python\n",
    "      * this will also make it easier to add in other years if I wanted to expand the analysis\n",
    "      \n",
    "As a first pass, I am going to focus only on the year __2013__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "mta_url = 'http://web.mta.info/developers/turnstile.html'\n",
    "db_name = 'TURNSTILES.db'\n",
    "table_name = 'thirteen'\n",
    "headers = [\"CA\", \"UNIT\", \"SCP\", \"STATION\", \"LINE\", \"DIVISION\", \"DATE\", \"TIME\", \"DESC\", \"ENTRIES\", \"EXITS\"]\n",
    "types = [\"TEXT\", \"TEXT\", \"TEXT\", \"TEXT\", \"TEXT\", \"TEXT\", \"DATE\", \"TIME\", \"TEXT\", \"INTEGER\", \"INTEGER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Scrape list of text file links from website and store in dictionary\n",
    " \n",
    " * Using Requests & BeautifulSoup to get url data and then parse html content\n",
    " * I am going to get all of the links currently present, and from there I can filter for the year I want\n",
    "     + From inspecting the html I noticed that the files I want all of the pattern \"data/nyct/turnstile/turnstile\" in the links, so I am going to use regular expressions to get only the links that I want \n",
    "     + From a ctrl+f I know that there are 301 occurrences of my pattern, so I'm expecting 301 links to be in my dictionary\n",
    "     + From here, I can either move forward with the full dictionary if I want all the links, or for this purpose I can call another function to create a smaller dictionary of only the links of interest (then I can delete the larger dictionary if I no longer want it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 links in full dictionary\n",
      "52 links in 2013 dictionary\n"
     ]
    }
   ],
   "source": [
    "page_data = requests.get(mta_url)\n",
    "pg_dat = page_data.content\n",
    "content = BeautifulSoup(pg_dat, 'html.parser')  \n",
    "page_links = content.find_all(\"a\", href=True)  \n",
    "\n",
    "t_links = {}\n",
    "for link in page_links:  \n",
    "    if re.match('^data/nyct/turnstile/turnstile.*', link['href']):  \n",
    "        t_links[link.text] = 'http://web.mta.info/developers/' + link['href']\n",
    "print len(t_links), 'links in full dictionary'\n",
    "\n",
    "\n",
    "'''\n",
    "get only links for 2013\n",
    "'''\n",
    "\n",
    "\n",
    "def year_links(url_dict, year):\n",
    "    y_links = {}\n",
    "    url_dict = t_links\n",
    "    for key in url_dict:\n",
    "        key_year = key.split(',')[2].split()\n",
    "        if str(year) in key_year:\n",
    "            y_links[key] = url_dict[key]\n",
    "    return y_links\n",
    "\n",
    "year = 2013\n",
    "y_links = year_links(t_links, year)\n",
    "\n",
    "print len(y_links), 'links in %d dictionary' % year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up database and functions to populate database\n",
    "* I am going to take advantage of sqlite in python to create a database with a table called 'thirteen' to oranize the date as I read it in\n",
    "* I know that I need a function to reformat the older files so that they match the newer files and fit into my table structure\n",
    "  + I need to separate the multiple entries that currently exist on one line of the older files\n",
    "  + I want to keep the C/A, UNIT, and SCP variables at the beginning of each line, and then repeat those for the other entries \n",
    "  + I need to add in a null value for the missing variables: station, linename, and division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "set up my database mta.db and create table thirteen\n",
    "'''\n",
    "\n",
    "\n",
    "def database_setup(db_name, table_name):\n",
    "\n",
    "    header_and_types = \"id INTEGER PRIMARY KEY AUTOINCREMENT, \" + \", \".join([headers[i] + ' ' +\n",
    "                                                                             types[i] for i in range(len(headers))])\n",
    "    global conn, cursor\n",
    "    conn = sql.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    create_string = 'CREATE TABLE IF NOT EXISTS '+ table_name + ' (' + header_and_types + ')'\n",
    "    cursor.execute(create_string)\n",
    "    return\n",
    "\n",
    "# run this function to set up database, commit, and then close connection for now\n",
    "database_setup(db_name, table_name)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "create function to update format of older files - only older files will be passed to this function\n",
    "'''\n",
    "\n",
    "def update_format(row):\n",
    "    \n",
    "    out_row = []  \n",
    "    len_id = 3  # chunk of 3 ID variables want to keep & repeat\n",
    "    len_entry = 5  # chunk the entries within the currently longer row\n",
    "    cells = row.split(',')  \n",
    "    \n",
    "    id_vars = \",\".join(cells[0:len_id])\n",
    "\n",
    "    # match to dataframe to get station, linename, division info (if available) \n",
    "    remote = cells[1]\n",
    "    booth = cells[0] \n",
    "\n",
    "    find_station = df.Station[df.Remote == remote][df.Booth == booth].values\n",
    "    station = find_station[0] if len(find_station) > 0 else 'NULL'\n",
    "\n",
    "    find_linename = df.Line[df.Remote == remote][df.Booth == booth].values\n",
    "    linename = find_linename[0] if len(find_linename) > 0 else 'NULL'\n",
    "\n",
    "    find_division = df.Division[df.Remote == remote][df.Booth == booth].values\n",
    "    division = find_division[0] if len(find_division) > 0 else 'NULL'\n",
    "    \n",
    "    for entry in range(len_id, len(cells), len_entry):\n",
    "        \n",
    "        # make date match format of newer files\n",
    "        date = datetime.strftime(datetime.strptime(cells[entry], \"%m-%d-%y\"), \"%m/%d/%Y\")\n",
    "\n",
    "        added_info = \",\".join([station, linename, division, date])\n",
    "        cleaned_row = id_vars + ',' + added_info + ',' + ','.join(cells[entry + 1: entry + len_entry])\n",
    "        \n",
    "        out_row.append(cleaned_row)\n",
    "        \n",
    "    return out_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "create function to add entry lines from text files and populate the database\n",
    "'''\n",
    "\n",
    "def add_entry(input_line, table_name):\n",
    "\n",
    "    values = input_line.split(',')\n",
    "    values = [j.strip() if j != 'NULL' else None for j in values]  # for null values, replace with none\n",
    "    values.insert(0, None)  # add empty value for id/primary key - auto populate\n",
    "    value_slots = \",\".join(['?']*(len(headers)+1))\n",
    "    insert_string = 'INSERT INTO ' + table_name + ' VALUES(' + value_slots + ')'\n",
    "\n",
    "    if len(values) == 12:\n",
    "        cursor.execute(insert_string, values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "create function to loop through the links in my dictionary and populate the database\n",
    "'''\n",
    "\n",
    "\n",
    "def populate_db(url_dict, db_name, table_name, outfile_name):\n",
    "  \n",
    "    # read in .csv of station keys for old files\n",
    "    global df\n",
    "    df = pd.read_csv('turnstiles_remote_station_key.csv')\n",
    "    df.columns = ['Remote', 'Booth', 'Station', 'Line', 'Division']\n",
    "\n",
    "    with open(outfile_name, 'w') as outfile:  # text file to track progress\n",
    "        \n",
    "        ctr = 1\n",
    "        outfile.write('Populating MTA database table turnstiles on' + datetime.now().ctime() + '\\n')\n",
    "\n",
    "        global conn, cursor\n",
    "        conn = sql.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "\n",
    "        for key in url_dict:\n",
    "            url = url_dict[key]\n",
    "\n",
    "            data = requests.get(url)\n",
    "            header = next(data.iter_lines()).split(\",\")\n",
    "            lines = data.iter_lines()\n",
    "            \n",
    "            if len(header) > 11:\n",
    "                for line in lines:\n",
    "                    entries = update_format(line)  \n",
    "\n",
    "                    for entry in entries:\n",
    "                        add_entry(entry, table_name)  \n",
    "\n",
    "            else:\n",
    "                lines.next()\n",
    "                for line in lines:\n",
    "                    if len(line) > 1:\n",
    "                        add_entry(line, table_name)\n",
    "                        \n",
    "            outfile.write('populated data for file number ' + str(ctr) + ': ' + url_dict[key] + '\\n')\n",
    "            print 'populated data for file number ' + str(ctr) + ': ' + url_dict[key]\n",
    "            ctr += 1\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execute database population function to scrape, process, and upload text files\n",
    "   * Call populate function with dictionary of links, database name, table name, and a file name to track which files were entered\n",
    "       + The populate_db function will output a text file with a statement that the file was uploaded if any lines are uploaded (or really just didn't throw errors). This doesn't necessarily reflect completeness of upload for that file. \n",
    "           - Come back to fix this\n",
    "       + The populate_db does take a non trivial amount of time to execute for a year's worth of data\n",
    "           - Come back and use trace function to see which steps could be cleaned up to shave off time. \n",
    "           - This would be particularly important if I wanted to add more years to my dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " populate_db(y_links, db_name, table_name, 'downloading_2013_turnstile_data_02062016.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Data Analysis\n",
    "    \n",
    "## Challenges and issues to keep in mind:\n",
    "From initial exploration fo the data I have noticed a few issues that have the potential to create big problems for the analysis (* Note this is only what I've found __so far__* ):\n",
    "\n",
    "1. The entries and exits values represent an \"odometer\" so when I want to get the value for any given day, I need to do a subtraction. \n",
    "2. Not all station names are unique values, and some stations serve multiple divisions\n",
    "3. There are a variety of different types of report descriptions - for the sake of simplicity, I am choosing to use only \"Regular\" reports\n",
    "4. Unique turnstiles are represented by combinations of Station, Booth, and SCP\n",
    "    + Ideally, I would have made a primary key for my database that reflected \"turnstile id\" - since I didn't do this, I'm going to have to group by a number of factors to prevent myself from double counting\n",
    "5. There is variation in the timing of turnstile reports\n",
    "    + Stations generally report over fixed intervals, but the timing varies by station. So one station may start reporting at midnight and then proceed every four hours, while another may start at 2 AM\n",
    "    + Some stations report more frequently than other stations\n",
    "6. There are instances where a turnstile seems to malfunction or \"reset\" and it's not always clear why this happened \n",
    "    + For instance, for the 08/01/2013 data, a turnstile in the 42 St Bryant Park station appears to reset between 8:00 and 12:00. It's unclear why this happens, and I don't know when this happened within the time interval\n",
    "    + In other cases, a turnstile may report a very large negative number, and the meaning of this valuue is unclear. For the sake of this analysis, I am excluding negative values \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sql.connect(db_name)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Which station has the most number of turnstile units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_stiles_query = \"SELECT COUNT(DISTINCT SCP) as turnstiles, STATION, DIVISION, CA FROM thirteen \" \\\n",
    "                   \"WHERE STATION is not NULL GROUP BY STATION, DIVISION ORDER BY turnstiles DESC ;\"\n",
    "\n",
    "top_units = pd.read_sql_query(num_stiles_query, conn)\n",
    "\n",
    "top_units[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penn_units = top_units['turnstiles'][top_units['STATION']== '34 ST-PENN STA'].sum()\n",
    "time_sq_units = pd.concat([top_units['turnstiles'][top_units['STATION'] == '42 ST-TIMES SQ'],\n",
    "                           top_units['turnstiles'][top_units['STATION'] == '42 ST-PA BUS TE']]).sum()\n",
    "\n",
    "print 'The Times Square Station complex has {0} turnstiles, and the Penn Station complex has {1} turnstiles '.format(\n",
    "    *[time_sq_units, penn_units])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of answer and approach\n",
    "* Defining station as distinct station name and division, the __IND__ division of __Penn Station__ has the most (__43__) units.\n",
    "\n",
    "\n",
    "* Taking into account that __Penn Station__ and __Times Square__ are both stations that serve multiple divisions, which appear in the top 10 counts of units per station, I considered these stations as complex stations and added the turnstile counts across divisions. Additionally, I combined the __Times Sq__ and __Port Authority Bus Terminal__ into one station given that the MTA website considers this one station complex.\n",
    "    + The __ Times Square complex__ has __89__ turnstiles\n",
    "    + The __Penn Station complex__ has __83__ turnstiles\n",
    "    \n",
    "    \n",
    "*  __42 St Times Square__ and the __42 St Port Authority__, considered as one station, have the *most* turnstile units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What was the total number of entries and exits across the subway system for 08/01/2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Query database and organize into dataframes\n",
    "'''\n",
    "# query for entry/exit counts for the last time point of the day\n",
    "end_time_query = \"SELECT STATION, DIVISION, CA, SCP, timestamp as ENDTIME, ENTRIES, EXITS from \" \\\n",
    "                 \"(SELECT MAX(TIME) as timestamp, ENTRIES, EXITS, SCP, CA, DIVISION, STATION from \" \\\n",
    "                 \"(SELECT * FROM thirteen WHERE  DATE = '08/01/2013' AND DESC = 'REGULAR' AND ENTRIES >= 0 AND \" \\\n",
    "                 \"EXITS >= 0 GROUP BY STATION, DIVISION, SCP, CA, TIME) GROUP BY STATION, DIVISION, CA, SCP) \" \\\n",
    "                 \"GROUP BY STATION, DIVISION, CA, SCP;\"\n",
    "\n",
    "# create data frame from query\n",
    "end_time = pd.read_sql_query(end_time_query, conn)\n",
    "\n",
    "# query for the entry/exit counts for the first time point of the day\n",
    "start_time_query = \"SELECT STATION, DIVISION, CA, SCP, timestamp as STARTTIME, ENTRIES, \" \\\n",
    "                    \"EXITS from (SELECT MIN(TIME) as timestamp, \" \\\n",
    "                   \"ENTRIES, EXITS, SCP, CA, DIVISION, STATION from (SELECT * FROM thirteen WHERE \" \\\n",
    "                    \"DATE = '08/01/2013' AND \" \\\n",
    "                   \"DESC = 'REGULAR' AND ENTRIES >= 0 AND EXITS >= 0 GROUP BY STATION, DIVISION, SCP, CA, TIME) \" \\\n",
    "                   \"GROUP BY STATION, DIVISION, CA, SCP) GROUP BY STATION, DIVISION, CA, SCP;\"\n",
    "\n",
    "# create data frame from query\n",
    "start_time = pd.read_sql_query(start_time_query, conn)\n",
    "\n",
    "# define categorical variables that I am going to use later for grouping\n",
    "start_time['STATION'] = start_time['STATION'].astype('category')\n",
    "start_time['SCP'] = start_time['SCP'].astype('category')\n",
    "start_time['DIVISION'] = start_time['DIVISION'].astype('category')\n",
    "\n",
    "end_time['STATION'] = end_time['STATION'].astype('category')\n",
    "end_time['SCP'] = end_time['SCP'].astype('category')\n",
    "end_time['DIVISION'] = end_time['DIVISION'].astype('category')\n",
    "\n",
    "'''\n",
    "Define total entries and exits for each day, and deal with \"issues\" (ie turnstile resets)\n",
    "'''\n",
    "entries = end_time['ENTRIES'] - start_time['ENTRIES']\n",
    "\n",
    "entries[entries < 0] # in fact row 1018 is a problem\n",
    "\n",
    "# let's check it out\n",
    "start_time.iloc[1018]\n",
    "end_time.iloc[1018]\n",
    "\n",
    "# replace entry total with approximation of daily entries, based on value after reset\n",
    "entries[1018] = end_time.iloc[1018]['ENTRIES']\n",
    "\n",
    "# total number of entries on this day\n",
    "total_entries = sum(entries)\n",
    "\n",
    "# now do the same for exits\n",
    "\n",
    "exits = end_time['EXITS'] - start_time['EXITS']\n",
    "exits[exits < 0] # again 1018 is a problem b/c of the reset\n",
    "\n",
    "exits[1018] = end_time.iloc[1018]['EXITS']\n",
    "total_exits = sum(exits)\n",
    "\n",
    "print 'approximate number of entries on 08/01/2013: ', total_entries\n",
    "\n",
    "print 'approximate number of exits on 08/01/2013: ', total_exits\n",
    "\n",
    "print 'approximate total amount of activity on 08/01/2013', total_exits + total_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of approach and answer:\n",
    "* I decided to do two separate queries to get entry/exit counts for the \"first\" and \"last\" regular report that day\n",
    "    + I ruled out negative counts\n",
    "    + one limitation of defining \"first\" and \"last\" based on min/max time stamps is that I could have happened to catch the turnstile at a moment of malfunction, whereas a different time stamp could have been more accurate\n",
    "    + this approach could have potentially been done in one, more complex, query; however I chose to try to err on the side of interpretability and also to give myself more 'access' to the data to try to deal with flaws\n",
    "    \n",
    "    \n",
    "* I chose to deal with the \"reset\" turnstile at Bryant Park by assigning that entry/exit count to the end value of the turnstile (after the reset)\n",
    "    + another approach could have been to replace that turnstile's count with the mean of the other counts at that station on 08/01/2013\n",
    "    \n",
    "    \n",
    "* The number of entries using this approach was __5,043,709__ and the number of exits using this approach was __3,936,543__\n",
    "    + This gives the total amount of activity as __8,980,252__\n",
    "    + To check if this order of magnitude was realistic, I decided to quantify a \"rider\" as one entry and exit. This means that my approach gives an approximation of __4,490,126 riders in one day__, which suggests that the subway system serves __1,638,895,990 riders a year__. This is of the order of magnitude of numbers reported by the MTA website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining busy-ness as the sume of entry and exit count:\n",
    "\n",
    "#### Which station was busiest on 08/01/2013?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time[\"BUSY\"] = entries + exits # note that the name of the df is no longer meaningful, interested in busy column\n",
    "\n",
    "# look at the top 10 stations in terms of busy-ness\n",
    "busy_by_station = start_time.groupby(['STATION', 'DIVISION'])['BUSY'].sum()\n",
    "busy_by_station.sort_values(ascending = False)[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining station as having a distinct name and division, the busiest station is __Grand Central Station__ with a total turnstile count of __294,257__ turns. However, I know that __Penn Station__ and __Times Square__ stations serve multiple divisions. So I am going to look at the business for both of these stations, combining across divisions (and in the case of Times Square, the other division has a different station name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_sq = pd.concat([start_time[start_time['STATION'] == '42 ST-TIMES SQ'],  start_time[start_time['STATION'] =='42 ST-PA BUS TE']])\n",
    "time_busy = time_sq['BUSY'].sum()\n",
    "\n",
    "penn_sq = start_time[start_time['STATION'] == '34 ST-PENN STA']\n",
    "penn_busy = penn_sq['BUSY'].sum()\n",
    "\n",
    "print 'Times Square had {0} total turns and Penn Station had {1} total turns'.format(*[time_busy, penn_busy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I take into account the fact that these hub stations serve multiple divisions, __Times Square__ is the busiest station with a total of __340,644__ turns. Additionally, Penn Station has __305,771__ turns, which is *greater* than the number of turns for __Grand Central Station__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which specific turnstile was busiest on 08/01/2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# identify the busiest turnstile\n",
    "busy_turnstile = start_time[start_time.BUSY == max(start_time.BUSY)]\n",
    "t_vals_print = [busy_turnstile['CA'].values[0], busy_turnstile['SCP'].values[0],busy_turnstile['STATION'].values[0], \n",
    "                busy_turnstile['BUSY'].values[0]]\n",
    "print 'the busiest turnstile was {0} {1} in station {2} with {3} turns'.format(*t_vals_print) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The busiest turnstile on 08/01/2013 was __N063a 00-00-00__ in __42 ST Port Authority__ with __10,546__ turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Identify the stations that have seen the most growth and decline in usage in 2013\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Set Up:\n",
    "* I created a new table 'daily' of daily entry/exit counts for each station by turnstile\n",
    "    + This helped to prevent my queries from getting too complex and convoluted\n",
    "    + I also will need to draw on this information moving forward\n",
    "    \n",
    "    \n",
    "* with this new table, I was able to get a total entry and exit count, as well as a count of busy-ness for each day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_daily_table = \"CREATE TABLE daily as select * from (SELECT min(end), DATE, entryend, exitend, \" \\\n",
    "                        \"SCP, CA, DIVISION, STATION from (SELECT MAX(TIME) as end, DATE, \"\\\n",
    "                        \"ENTRIES as entryend, EXITS as exitend, SCP, CA, DIVISION, STATION from \" \\\n",
    "                        \"(SELECT * FROM thirteen WHERE  DESC = 'REGULAR' AND ENTRIES >= 0 AND \"\n",
    "                        \"EXITS >= 0 AND STATION IS NOT NULL GROUP BY STATION, DIVISION, SCP, CA, DATE, TIME) \"\n",
    "                        \"GROUP BY DATE, STATION, DIVISION, CA, SCP \" \\\n",
    "                        \"UNION \" \\\n",
    "                        \"SELECT MIN(TIME) as start, DATE, ENTRIES as entrystart, \" \\\n",
    "                        \"EXITS as exitstart, SCP, CA, DIVISION, STATION from \" \\\n",
    "                        \"(SELECT * FROM thirteen WHERE  DESC = 'REGULAR' AND ENTRIES >= 0 AND \" \\\n",
    "                        \"EXITS >= 0 AND STATION IS NOT NULL GROUP BY STATION, DIVISION, SCP, CA, DATE, TIME) \"\\\n",
    "                        \"GROUP BY DATE, STATION, DIVISION, CA, SCP \"\\\n",
    "                        \"ORDER BY STATION, SCP) \" \\\n",
    "                        \"GROUP BY DATE, STATION, DIVISION, SCP ORDER BY STATION, SCP);\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptualizing Growth and Decline:\n",
    "* Given that I have an 'odometer' type reading of turns (busy-ness), I should expect that the turns should increase each day, except when I have a reset or some other disruption. I decided to quantify turns per day, and then look for changes in turns per day. I define __growth as an average positive change in turns per day__, and __decline as an average negative change in turns per day__.\n",
    "\n",
    "\n",
    "* To quantify turns per day, I used the diff() function in pandas to subtract the previous day's turn count from the current day, for each station. \n",
    "    + This method could result in negative or disproportionately large difference in the case of disruption/reset. \n",
    "    + I dropped these instances by setting all negative first order differences to NaN. \n",
    "    + Also, I know that the busiest stations see turns per day in the hundreds of thousands range, and so any count of turns per day significantly higher than this is likely noise. I set a liberal threshold of 800K for turns per day, and any counts exceeding this threshold were dropped (set to NaN).\n",
    "\n",
    "\n",
    "* To quantify __change in turns per day__, I used the diff() function again to quantify the fluctuations in rates of turns per day. I then took the average of this quantity for each station.\n",
    "    + If a station experiences a constant rate of turns per month, this quantity should be about zero\n",
    "    + if the rate of turns per month goes down, signaling __decline__ in usage, this quantity will be negative\n",
    "    + If the rate of turns per month increases, signaling __growth__ in usage, this quantity will be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "query, pull in data frame, format categories\n",
    "'''\n",
    "all_days_query = \"SELECT STATION, DIVISION, DATE, sum(entryend) as T_ENTRIES, sum(exitend) as T_EXITS, \" \\\n",
    "                 \"sum(entryend) + sum(exitend) as BUSY from daily GROUP BY DATE, STATION, DIVISION \" \\\n",
    "                 \"ORDER BY STATION, DIVISION ;\"\n",
    "\n",
    "days_df = pd.read_sql_query(all_days_query, conn)\n",
    "\n",
    "days_df['STATION'] = days_df['STATION'].astype('category')\n",
    "days_df['DIVISION'] = days_df['DIVISION'].astype('category')\n",
    "\n",
    "'''\n",
    "take difference between days of 'busy' to get turns per day - grouped by station/division\n",
    "'''\n",
    "days_df['DIFF'] = days_df.groupby(['STATION', 'DIVISION'])['BUSY'].diff()\n",
    "\n",
    "# drop noisy counts (resets, disturbances, etc)\n",
    "days_df['DIFF'] = days_df['DIFF'].where(days_df['DIFF'] >= 0, 'NaN').astype('float')\n",
    "days_df['DIFF'] = days_df['DIFF'].where(days_df['DIFF'] < 800000, 'NaN').astype('float')\n",
    "\n",
    "'''\n",
    "sanity check: are my busiest stations reporting the highest number of turns per day?\n",
    "'''\n",
    "avg_diff = days_df.groupby(['STATION', 'DIVISION'])['DIFF'].mean()\n",
    "avg_diff.sort_values(ascending = False)[0:10]\n",
    "\n",
    "\n",
    "'''\n",
    "get change in turns per day, and average change - grouped by station/division\n",
    "'''\n",
    "\n",
    "days_df['DIFF2'] = days_df.groupby(['STATION', 'DIVISION'])['DIFF'].diff()\n",
    "\n",
    "avg_2nd_diff = days_df.groupby(['STATION', 'DIVISION'])['DIFF2'].mean()\n",
    "\n",
    "# which stations are declining?\n",
    "avg_2nd_diff.sort_values()[0:5]\n",
    "\n",
    "#growing?\n",
    "avg_2nd_diff.sort_values(ascending = False)[0:5]\n",
    "\n",
    "'''# look at Z's'''\n",
    "\n",
    "z_2nd_vals = (avg_2nd_diff - avg_2nd_diff.mean())/avg_2nd_diff.std()\n",
    "z_2nd_vals.sort_values()[0:5]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
